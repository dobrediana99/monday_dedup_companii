#!/usr/bin/env python3
"""
Safe, deterministic company deduplication for monday.com boards.

Board A (Companies): 1853156756
Board B (Orders / Transports): 2030349838
Board C (Contacts): 1853156713

Relations on Board B:
  - board_relation_mkpw4bcs (Companie Client)
  - board_relation_mkse9rp2 (Companie Furnizor)

Execution modes:
  1) dry-run  -> no mutations, produces impact report
  2) execute  -> migrates relations, verifies, archives duplicates

Usage examples:
  export MONDAY_API_TOKEN="..."
  python monday_dedup_companii.py --mode dry-run
  python monday_dedup_companii.py --mode execute
"""

from __future__ import annotations

import argparse
import datetime as dt
import json
import os
import re
import sys
import time
import unicodedata
import urllib.error
import urllib.request
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Sequence, Tuple


COMPANIES_BOARD_ID = 1853156756
ORDERS_BOARD_ID = 2030349838
CONTACTS_BOARD_ID = 1853156713

COMPANIES_VAT_COLUMN_ID = "text_mknrvv8q"
COMPANIES_EMAIL_COLUMN_ID = "email_mkq9wa9b"
COMPANIES_CONTACTS_RELATION_COLUMN_ID = "board_relation_mkpdn5kt"
CONTACT_PHONE_COLUMN_ID = "contact_phone"

RELATION_COLUMNS = ("board_relation_mkpw4bcs", "board_relation_mkse9rp2")
MONDAY_API_URL = "https://api.monday.com/v2"

LEGAL_SUFFIXES = {
    "SRL",
    "GMBH",
    "SPA",
    "SA",
    "LTD",
    "LLC",
    "INC",
    "BV",
    "NV",
    "PLC",
    "CO",
    "COMPANY",
    "SAS",
    "SRLS",
    "KG",
    "AG",
    "OY",
    "AB",
    "AS",
    "KFT",
    "OOO",
    "SRO",
    "EURL",
    "SARL",
    "SL",
    "SR",
}


class MondayApiError(RuntimeError):
    """Raised when monday.com API calls fail permanently."""


class SafetyCheckError(RuntimeError):
    """Raised when an operation could cause data loss."""


class UnionFind:
    """Disjoint Set for clustering overlapping company keys deterministically."""
    def __init__(self) -> None:
        self.parent: Dict[int, int] = {}

    def find(self, i: int) -> int:
        if self.parent.setdefault(i, i) != i:
            self.parent[i] = self.find(self.parent[i])
        return self.parent[i]

    def union(self, i: int, j: int) -> None:
        root_i = self.find(i)
        root_j = self.find(j)
        if root_i != root_j:
            # Deterministic: make the smaller root the parent
            if root_i < root_j:
                self.parent[root_j] = root_i
            else:
                self.parent[root_i] = root_j


@dataclass(frozen=True)
class ColumnMeta:
    id: str
    title: str
    type: str
    settings_str: str


@dataclass
class CompanyRecord:
    id: int
    name: str
    created_at: str
    created_at_dt: dt.datetime
    column_values: Dict[str, Dict[str, Any]]
    vat_display: str
    vat_key: str
    email_display: str
    email_keys: Tuple[str, ...]
    contact_ids: Tuple[int, ...]
    phone_source_contact_id: Optional[int]
    phone_display: str
    phone_key: str
    normalized_name: str


@dataclass(frozen=True)
class DuplicateGroup:
    matched_keys: Tuple[str, ...]
    canonical_id: int
    duplicate_ids: Tuple[int, ...]
    all_ids: Tuple[int, ...]


@dataclass(frozen=True)
class MigrationAction:
    order_id: int
    column_id: str
    from_company_id: int
    to_company_id: int


class MondayClient:
    def __init__(
        self,
        api_token: str,
        api_url: str = MONDAY_API_URL,
        max_retries: int = 5,
        backoff_seconds: float = 1.0,
        timeout_seconds: float = 60.0,
    ) -> None:
        self.api_token = api_token
        self.api_url = api_url
        self.max_retries = max_retries
        self.backoff_seconds = backoff_seconds
        self.timeout_seconds = timeout_seconds

    def execute(self, query: str, variables: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        payload = {"query": query, "variables": variables or {}}
        encoded_payload = json.dumps(payload).encode("utf-8")
        headers = {
            "Authorization": self.api_token,
            "Content-Type": "application/json",
        }

        for attempt in range(self.max_retries + 1):
            request = urllib.request.Request(
                self.api_url,
                data=encoded_payload,
                headers=headers,
                method="POST",
            )
            try:
                with urllib.request.urlopen(request, timeout=self.timeout_seconds) as response:
                    body = response.read().decode("utf-8")
                    status = response.status
            except urllib.error.HTTPError as error:
                body = ""
                if error.fp is not None:
                    try:
                        body = error.read().decode("utf-8")
                    except Exception:
                        body = ""
                if self._retry_http(error.code, attempt):
                    self._sleep_with_backoff(attempt, None)
                    continue
                raise MondayApiError(f"HTTP {error.code} from monday.com: {body or str(error)}") from error
            except urllib.error.URLError as error:
                if attempt < self.max_retries:
                    self._sleep_with_backoff(attempt, None)
                    continue
                raise MondayApiError(f"Network error from monday.com: {error}") from error

            if status == 429 or status >= 500:
                if attempt < self.max_retries:
                    self._sleep_with_backoff(attempt, None)
                    continue
                raise MondayApiError(f"HTTP {status} from monday.com after retries.")

            try:
                payload_json = json.loads(body)
            except json.JSONDecodeError as error:
                if attempt < self.max_retries:
                    self._sleep_with_backoff(attempt, None)
                    continue
                raise MondayApiError(f"Invalid JSON response from monday.com: {body}") from error

            gql_errors = payload_json.get("errors") or []
            if gql_errors:
                retry_after = self._extract_retry_after_seconds(gql_errors)
                if self._retry_graphql(gql_errors, attempt):
                    self._sleep_with_backoff(attempt, retry_after)
                    continue
                raise MondayApiError(f"GraphQL errors from monday.com: {json.dumps(gql_errors, ensure_ascii=True)}")

            return payload_json.get("data") or {}

        raise MondayApiError("monday.com request retries exhausted.")

    def _retry_http(self, http_code: int, attempt: int) -> bool:
        return http_code in {408, 409, 425, 429} or (http_code >= 500 and attempt < self.max_retries)

    def _retry_graphql(self, errors: List[Dict[str, Any]], attempt: int) -> bool:
        if attempt >= self.max_retries:
            return False

        retryable_tokens = (
            "rate",
            "limit",
            "complexity",
            "timeout",
            "concurrency",
            "temporar",
            "throttl",
        )
        for error in errors:
            message = str(error.get("message", "")).lower()
            code = str(error.get("extensions", {}).get("code", "")).lower()
            haystack = f"{message} {code}"
            if any(token in haystack for token in retryable_tokens):
                return True
        return False

    def _extract_retry_after_seconds(self, errors: List[Dict[str, Any]]) -> Optional[float]:
        for error in errors:
            extensions = error.get("extensions", {})
            for key in ("retry_in_seconds", "retry_after", "retryAfter", "retryInSeconds"):
                value = extensions.get(key)
                if value is None:
                    continue
                try:
                    parsed = float(value)
                    if parsed > 0:
                        return parsed
                except (TypeError, ValueError):
                    continue
        return None

    def _sleep_with_backoff(self, attempt: int, retry_after_seconds: Optional[float]) -> None:
        if retry_after_seconds is not None:
            sleep_seconds = retry_after_seconds
        else:
            sleep_seconds = self.backoff_seconds * (2 ** attempt)
        time.sleep(sleep_seconds)


BOARD_COLUMNS_QUERY = """
query BoardColumns($boardId: [ID!]) {
  boards(ids: $boardId) {
    id
    columns {
      id
      title
      type
      settings_str
    }
  }
}
"""

FIRST_PAGE_ITEMS_QUERY = """
query FirstPageItems($boardId: [ID!], $limit: Int!, $columnIds: [String!]) {
  boards(ids: $boardId) {
    id
    items_page(limit: $limit) {
      cursor
      items {
        id
        name
        created_at
        column_values(ids: $columnIds) {
          id
          type
          value
          text
          ... on BoardRelationValue {
            linked_item_ids
          }
        }
      }
    }
  }
}
"""

NEXT_PAGE_ITEMS_QUERY = """
query NextPageItems($cursor: String!, $limit: Int!, $columnIds: [String!]) {
  next_items_page(cursor: $cursor, limit: $limit) {
    cursor
    items {
      id
      name
      created_at
      column_values(ids: $columnIds) {
        id
        type
        value
        text
        ... on BoardRelationValue {
          linked_item_ids
        }
      }
    }
  }
}
"""

CHANGE_COLUMN_VALUE_MUTATION = """
mutation ChangeRelation($boardId: ID!, $itemId: ID!, $columnId: String!, $value: JSON!) {
  change_column_value(
    board_id: $boardId,
    item_id: $itemId,
    column_id: $columnId,
    value: $value
  ) {
    id
  }
}
"""

ARCHIVE_ITEM_MUTATION = """
mutation ArchiveItem($itemId: ID!) {
  archive_item(item_id: $itemId) {
    id
  }
}
"""


def remove_diacritics(text: str) -> str:
    decomposed = unicodedata.normalize("NFKD", text)
    return "".join(ch for ch in decomposed if not unicodedata.combining(ch))


def normalize_company_name(name: str) -> str:
    ascii_name = remove_diacritics((name or "").upper())
    ascii_name = re.sub(r"[^A-Z0-9\s]", " ", ascii_name)

    # Normalize dotted/acronym legal suffixes into canonical tokens.
    canonicalized = re.sub(r"\bS\s*R\s*L\b", " SRL ", ascii_name)
    canonicalized = re.sub(r"\bL\s*L\s*C\b", " LLC ", canonicalized)
    canonicalized = re.sub(r"\bL\s*T\s*D\b", " LTD ", canonicalized)
    canonicalized = re.sub(r"\bS\s*A\b", " SA ", canonicalized)

    tokens = [token for token in canonicalized.split() if token and token not in LEGAL_SUFFIXES]
    return " ".join(tokens).strip()


def normalize_vat(vat_display: str) -> str:
    return re.sub(r"[^A-Z0-9]", "", remove_diacritics((vat_display or "").upper()))


def normalize_emails(email_display: str) -> List[str]:
    if not email_display:
        return []
    parts = re.split(r'[,;\s]+', email_display)
    valid_emails = []
    for p in parts:
        p = p.strip().lower()
        if p and "@" in p and "." in p:
            valid_emails.append(p)
    return sorted(list(set(valid_emails)))


def normalize_phone(phone_display: str) -> str:
    raw = (phone_display or "").strip()
    if not raw:
        return ""
    has_plus = raw.startswith("+")
    digits = re.sub(r"\D", "", raw)
    if not digits:
        return ""
    return f"+{digits}" if has_plus else digits


def extract_phone_from_column(col_data: Dict[str, Any]) -> str:
    """Robust fallback: try parsing phone from 'text' first, then 'value' JSON payload."""
    text = (col_data.get("text") or "").strip()
    if text:
        return text

    val_str = (col_data.get("value") or "").strip()
    if not val_str:
        return ""

    try:
        val_json = json.loads(val_str)
        if isinstance(val_json, dict):
            for key in ["phone", "text", "value"]:
                if key in val_json and isinstance(val_json[key], (str, int)):
                    return str(val_json[key])
            for v in val_json.values():
                if isinstance(v, str) and re.search(r"\d{4,}", v):
                    return v
        elif isinstance(val_json, list) and val_json:
            if isinstance(val_json[0], (str, int)):
                return str(val_json[0])
    except json.JSONDecodeError:
        pass

    return ""


def select_best_phone(phones: List[Tuple[int, str, str]]) -> Optional[Tuple[int, str, str]]:
    """
    phones: List of (contact_id, display_phone, normalized_phone)
    Returns canonical phone based on deterministic rules:
      1. Has '+' prefix
      2. Longest digit count
      3. Lexicographically smallest string
    """
    valid_phones = [p for p in phones if p[2]]
    if not valid_phones:
        return None

    def sort_key(p: Tuple[int, str, str]) -> Tuple[int, int, str]:
        norm_key = p[2]
        has_plus = 0 if norm_key.startswith("+") else 1  # 0 first (True), 1 later (False)
        digit_count = -len(re.sub(r"\D", "", norm_key))  # Smaller negative number first (longer)
        return (has_plus, digit_count, norm_key)         # String ascending as tie-breaker

    return sorted(valid_phones, key=sort_key)[0]


def parse_datetime(value: str) -> dt.datetime:
    raw = (value or "").strip()
    if not raw:
        return dt.datetime.max.replace(tzinfo=dt.timezone.utc)
    if raw.endswith("Z"):
        raw = raw[:-1] + "+00:00"
    try:
        parsed = dt.datetime.fromisoformat(raw)
    except ValueError:
        return dt.datetime.max.replace(tzinfo=dt.timezone.utc)
    if parsed.tzinfo is None:
        return parsed.replace(tzinfo=dt.timezone.utc)
    return parsed


def compact_json(value: Any) -> str:
    return json.dumps(value, separators=(",", ":"), ensure_ascii=True)


def fetch_board_columns(client: MondayClient, board_id: int) -> List[ColumnMeta]:
    data = client.execute(BOARD_COLUMNS_QUERY, {"boardId": [str(board_id)]})
    boards = data.get("boards") or []
    if not boards:
        raise MondayApiError(f"Board {board_id} not found.")
    raw_columns = boards[0].get("columns") or []
    columns = [
        ColumnMeta(
            id=str(col.get("id", "")).strip(),
            title=str(col.get("title", "")).strip(),
            type=str(col.get("type", "")).strip(),
            settings_str=str(col.get("settings_str", "") or ""),
        )
        for col in raw_columns
        if col.get("id")
    ]
    return columns


def fetch_all_items(
    client: MondayClient,
    board_id: int,
    column_ids: Sequence[str],
    page_limit: int = 500,
) -> List[Dict[str, Any]]:
    variables = {
        "boardId": [str(board_id)],
        "limit": page_limit,
        "columnIds": list(column_ids),
    }
    first_page_data = client.execute(FIRST_PAGE_ITEMS_QUERY, variables)
    boards = first_page_data.get("boards") or []
    if not boards:
        raise MondayApiError(f"Board {board_id} not found when fetching items.")

    items_page = boards[0].get("items_page") or {}
    items = list(items_page.get("items") or [])
    cursor = items_page.get("cursor")

    while cursor:
        next_page_data = client.execute(
            NEXT_PAGE_ITEMS_QUERY,
            {"cursor": cursor, "limit": page_limit, "columnIds": list(column_ids)},
        )
        next_page = next_page_data.get("next_items_page") or {}
        items.extend(next_page.get("items") or [])
        cursor = next_page.get("cursor")

    return items


def as_column_map(item: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    result: Dict[str, Dict[str, Any]] = {}
    for cv in item.get("column_values") or []:
        column_id = str(cv.get("id", "")).strip()
        if not column_id:
            continue
        result[column_id] = {
            "text": cv.get("text"),  # Strict string reading used in standard logic
            "value": cv.get("value"),
            "type": cv.get("type"),
            "linked_item_ids": cv.get("linked_item_ids") or [],
        }
    return result


def company_from_item(item: Dict[str, Any], contact_phones: Dict[int, str]) -> CompanyRecord:
    column_map = as_column_map(item)
    item_id = int(str(item.get("id", "0")))
    name = str(item.get("name", "")).strip()
    created_at = str(item.get("created_at", "")).strip()

    vat_display = (column_map.get(COMPANIES_VAT_COLUMN_ID, {}).get("text") or "").strip()
    vat_key = normalize_vat(vat_display)

    email_display = (column_map.get(COMPANIES_EMAIL_COLUMN_ID, {}).get("text") or "").strip()
    email_keys = normalize_emails(email_display)

    # Extract contact IDs connected to this company
    rel_col = column_map.get(COMPANIES_CONTACTS_RELATION_COLUMN_ID, {})
    contact_ids_raw = rel_col.get("linked_item_ids") or []
    contact_ids = [int(x) for x in contact_ids_raw if str(x).isdigit()]

    # Collect and select best phone from Contacts
    phones_to_evaluate = []
    for cid in contact_ids:
        p_disp = contact_phones.get(cid, "")
        p_key = normalize_phone(p_disp)
        if p_key:
            phones_to_evaluate.append((cid, p_disp, p_key))

    best_phone = select_best_phone(phones_to_evaluate)
    if best_phone:
        phone_source, phone_display, phone_key = best_phone
    else:
        phone_source, phone_display, phone_key = None, "", ""

    normalized_name = normalize_company_name(name)

    return CompanyRecord(
        id=item_id,
        name=name,
        created_at=created_at,
        created_at_dt=parse_datetime(created_at),
        column_values=column_map,
        vat_display=vat_display,
        vat_key=vat_key,
        email_display=email_display,
        email_keys=tuple(email_keys),
        contact_ids=tuple(contact_ids),
        phone_source_contact_id=phone_source,
        phone_display=phone_display,
        phone_key=phone_key,
        normalized_name=normalized_name,
    )


def company_score(record: CompanyRecord) -> int:
    """Score indicates how many identifiers are filled (0 to 3 max)."""
    score = 0
    if record.vat_key:
        score += 1
    if record.email_keys:
        score += 1
    if record.phone_key:
        score += 1
    return score


def select_canonical(records: Sequence[CompanyRecord]) -> CompanyRecord:
    def sort_key(record: CompanyRecord) -> Tuple[int, dt.datetime, int]:
        # High score -> Older creation date -> Smallest ID
        return (-company_score(record), record.created_at_dt, record.id)

    return sorted(records, key=sort_key)[0]


def build_duplicate_groups(companies: Sequence[CompanyRecord]) -> List[DuplicateGroup]:
    uf = UnionFind()
    key_to_ids: Dict[str, List[int]] = {}

    # Map each populated identifier key to the list of corresponding company IDs.
    for company in companies:
        keys = []
        if company.vat_key:
            keys.append(f"VAT:{company.vat_key}")
        for email in company.email_keys:
            keys.append(f"EMAIL:{email}")
        if company.phone_key:
            keys.append(f"PHONE:{company.phone_key}")
        if company.normalized_name:
            keys.append(f"NAME:{company.normalized_name}")

        for k in keys:
            key_to_ids.setdefault(k, []).append(company.id)

    # Union all companies sharing the same key.
    for ids in key_to_ids.values():
        if len(ids) > 1:
            first = ids[0]
            for other in ids[1:]:
                uf.union(first, other)

    # Group records by their UF root.
    clusters: Dict[int, List[CompanyRecord]] = {}
    for company in companies:
        root = uf.find(company.id)
        clusters.setdefault(root, []).append(company)

    groups: List[DuplicateGroup] = []
    for root, records in clusters.items():
        if len(records) < 2:
            continue

        # Compile debug strings of matching keys.
        cluster_ids = {r.id for r in records}
        matched_keys = []
        for k, ids_list in key_to_ids.items():
            intersect = cluster_ids.intersection(ids_list)
            if len(intersect) > 1:
                matched_keys.append(k)

        canonical = select_canonical(records)
        duplicate_ids = tuple(sorted(r.id for r in records if r.id != canonical.id))
        all_ids = tuple(sorted(r.id for r in records))

        groups.append(
            DuplicateGroup(
                matched_keys=tuple(sorted(matched_keys)),
                canonical_id=canonical.id,
                duplicate_ids=duplicate_ids,
                all_ids=all_ids,
            )
        )

    # Return groups sorted deterministically
    return sorted(groups, key=lambda group: group.canonical_id)


def parse_relation_ids(raw_value: Optional[str], context: str) -> List[int]:
    if not raw_value:
        return []
    try:
        parsed = json.loads(raw_value)
    except json.JSONDecodeError as error:
        raise SafetyCheckError(
            f"Failed to parse relation JSON for {context}. Raw value: {raw_value}"
        ) from error

    linked = parsed.get("linkedPulseIds")
    if linked is None:
        linked = parsed.get("item_ids")
    if linked is None:
        return []

    result: List[int] = []
    if isinstance(linked, list):
        for entry in linked:
            if isinstance(entry, dict):
                raw_id = entry.get("linkedPulseId")
                if raw_id is None:
                    raw_id = entry.get("linkedPulseID")
                if raw_id is None:
                    raw_id = entry.get("id")
            else:
                raw_id = entry
            if raw_id is None:
                continue
            try:
                result.append(int(str(raw_id)))
            except ValueError:
                continue
    return result


def check_relation_columns_are_single_select(order_columns: Sequence[ColumnMeta]) -> None:
    by_id = {column.id: column for column in order_columns}
    for column_id in RELATION_COLUMNS:
        column = by_id.get(column_id)
        if column is None:
            raise SafetyCheckError(f"Required relation column '{column_id}' not found on orders board.")
        settings_raw = column.settings_str or "{}"
        try:
            settings = json.loads(settings_raw)
        except json.JSONDecodeError:
            settings = {}
        allow_multiple = settings.get("allowMultipleItems")
        if allow_multiple is True:
            raise SafetyCheckError(
                f"Column '{column_id}' has allowMultipleItems=true. Script requires single-select relations."
            )


def plan_relation_migration(
    order_items: Sequence[Dict[str, Any]],
    duplicate_to_canonical: Dict[int, int],
) -> Tuple[List[MigrationAction], Dict[int, List[int]]]:
    actions: List[MigrationAction] = []
    affected_orders_by_duplicate: Dict[int, List[int]] = {}

    for item in order_items:
        order_id = int(str(item.get("id", "0")))
        order_columns = as_column_map(item)
        for column_id in RELATION_COLUMNS:
            col = order_columns.get(column_id, {})
            relation_ids_raw = col.get("linked_item_ids") or []
            relation_ids = [int(x) for x in relation_ids_raw if str(x).isdigit()]
            if len(relation_ids) > 1:
                raise SafetyCheckError(
                    f"Order {order_id} / column {column_id} contains multiple linked items ({relation_ids}), "
                    "but relation is expected to be single-select."
                )
            if not relation_ids:
                continue

            current_company_id = relation_ids[0]
            canonical_company_id = duplicate_to_canonical.get(current_company_id)
            if canonical_company_id is None:
                continue
            if canonical_company_id == current_company_id:
                continue

            actions.append(
                MigrationAction(
                    order_id=order_id,
                    column_id=column_id,
                    from_company_id=current_company_id,
                    to_company_id=canonical_company_id,
                )
            )
            affected_orders_by_duplicate.setdefault(current_company_id, [])
            affected_orders_by_duplicate[current_company_id].append(order_id)

    actions.sort(key=lambda action: (action.order_id, action.column_id, action.from_company_id, action.to_company_id))
    for duplicate_id, order_ids in affected_orders_by_duplicate.items():
        unique_sorted_ids = sorted(set(order_ids))
        affected_orders_by_duplicate[duplicate_id] = unique_sorted_ids
    return actions, affected_orders_by_duplicate


def execute_relation_updates(
    client: MondayClient,
    actions: Sequence[MigrationAction],
    orders_board_id: int,
) -> List[Dict[str, Any]]:
    applied: List[Dict[str, Any]] = []
    for action in actions:
        mutation_value = compact_json({"linkedPulseIds": [{"linkedPulseId": action.to_company_id}]})
        variables = {
            "boardId": str(orders_board_id),
            "itemId": str(action.order_id),
            "columnId": action.column_id,
            "value": mutation_value,
        }
        client.execute(CHANGE_COLUMN_VALUE_MUTATION, variables)
        applied.append(
            {
                "order_id": action.order_id,
                "column_id": action.column_id,
                "from_company_id": action.from_company_id,
                "to_company_id": action.to_company_id,
            }
        )
    return applied


def verify_no_duplicate_references(
    order_items: Sequence[Dict[str, Any]],
    duplicate_ids: Sequence[int],
) -> List[Dict[str, Any]]:
    duplicate_id_set = set(duplicate_ids)
    leftovers: List[Dict[str, Any]] = []

    for item in order_items:
        order_id = int(str(item.get("id", "0")))
        order_columns = as_column_map(item)
        for column_id in RELATION_COLUMNS:
            col = order_columns.get(column_id, {})
            relation_ids_raw = col.get("linked_item_ids") or []
            relation_ids = [int(x) for x in relation_ids_raw if str(x).isdigit()]
            for relation_id in relation_ids:
                if relation_id in duplicate_id_set:
                    leftovers.append(
                        {
                            "order_id": order_id,
                            "column_id": column_id,
                            "duplicate_id": relation_id,
                        }
                    )
    leftovers.sort(key=lambda row: (row["order_id"], row["column_id"], row["duplicate_id"]))
    return leftovers


def archive_duplicates(client: MondayClient, duplicate_ids: Sequence[int]) -> List[int]:
    archived: List[int] = []
    for duplicate_id in sorted(set(duplicate_ids)):
        client.execute(ARCHIVE_ITEM_MUTATION, {"itemId": str(duplicate_id)})
        archived.append(duplicate_id)
    return archived


def build_duplicate_map(groups: Sequence[DuplicateGroup]) -> Dict[int, int]:
    duplicate_to_canonical: Dict[int, int] = {}
    for group in groups:
        for duplicate_id in group.duplicate_ids:
            duplicate_to_canonical[duplicate_id] = group.canonical_id
    return duplicate_to_canonical


def build_company_index(companies: Sequence[CompanyRecord]) -> Dict[int, CompanyRecord]:
    return {company.id: company for company in companies}


def score_snapshot(record: CompanyRecord) -> Dict[str, Any]:
    return {
        "company_id": record.id,
        "name": record.name,
        "score": company_score(record),
        "vat": record.vat_display,
        "email": record.email_display,
        "contact_ids": list(record.contact_ids),
        "phone_source_contact_id": record.phone_source_contact_id,
        "phone_display": record.phone_display,
        "phone_key": record.phone_key,
        "created_at": record.created_at,
    }


def run(args: argparse.Namespace) -> Dict[str, Any]:
    token = args.api_token or os.getenv("MONDAY_API_TOKEN")
    if not token:
        raise SafetyCheckError("Missing monday.com API token. Use --api-token or MONDAY_API_TOKEN environment variable.")

    client = MondayClient(
        api_token=token,
        api_url=args.api_url,
        max_retries=args.max_retries,
        backoff_seconds=args.backoff_seconds,
        timeout_seconds=args.timeout_seconds,
    )

    report: Dict[str, Any] = {
        "mode": args.mode,
        "timestamp_utc": dt.datetime.now(dt.timezone.utc).isoformat(),
        "boards": {
            "companies_board_id": args.companies_board_id,
            "orders_board_id": args.orders_board_id,
            "contacts_board_id": args.contacts_board_id,
        },
        "relation_columns": list(RELATION_COLUMNS),
        "hardcoded_columns": {
            "vat": COMPANIES_VAT_COLUMN_ID,
            "email": COMPANIES_EMAIL_COLUMN_ID,
            "contacts_relation": COMPANIES_CONTACTS_RELATION_COLUMN_ID,
            "contact_phone": CONTACT_PHONE_COLUMN_ID,
        },
        "assumptions": {
            "single_select_relations_required": True,
            "read_human_values_from_text": True,
            "update_relations_with_linkedPulseIds_json_string": True,
        },
    }

    order_columns_meta = fetch_board_columns(client, args.orders_board_id)
    check_relation_columns_are_single_select(order_columns_meta)

    # 1. Fetch Contact Board Phones
    contact_items_raw = fetch_all_items(
        client=client,
        board_id=args.contacts_board_id,
        column_ids=[CONTACT_PHONE_COLUMN_ID],
        page_limit=args.page_limit,
    )
    contact_phones = {}
    for c_item in contact_items_raw:
        c_id = int(str(c_item.get("id", "0")))
        c_col_map = as_column_map(c_item)
        p_disp = extract_phone_from_column(c_col_map.get(CONTACT_PHONE_COLUMN_ID, {}))
        contact_phones[c_id] = p_disp

    # 2. Fetch Companies Data
    company_column_ids_to_fetch = [
        COMPANIES_VAT_COLUMN_ID,
        COMPANIES_EMAIL_COLUMN_ID,
        COMPANIES_CONTACTS_RELATION_COLUMN_ID,
    ]

    company_items_raw = fetch_all_items(
        client=client,
        board_id=args.companies_board_id,
        column_ids=company_column_ids_to_fetch,
        page_limit=args.page_limit,
    )

    # 3. Assemble and calculate Companies (with phone mapping)
    companies = [company_from_item(item, contact_phones) for item in company_items_raw]
    company_by_id = build_company_index(companies)

    duplicate_groups_all = build_duplicate_groups(companies)
    duplicate_groups_total_found = len(duplicate_groups_all)

    # 4. Apply Limit logic
    is_limited = args.max_duplicate_groups > 0 and duplicate_groups_total_found > args.max_duplicate_groups
    
    if args.max_duplicate_groups > 0:
        duplicate_groups = duplicate_groups_all[:args.max_duplicate_groups]
    else:
        duplicate_groups = duplicate_groups_all
        
    duplicate_groups_processed = len(duplicate_groups)

    report["limits"] = {
        "max_duplicate_groups": args.max_duplicate_groups,
        "duplicate_groups_total_found": duplicate_groups_total_found,
        "duplicate_groups_processed": duplicate_groups_processed,
        "is_limited": is_limited
    }

    if args.mode == "execute" and is_limited:
        report["warning"] = f"Execution is limited to {args.max_duplicate_groups} duplicate groups. Not all duplicates will be processed."

    duplicate_to_canonical = build_duplicate_map(duplicate_groups)
    duplicate_ids = sorted(duplicate_to_canonical.keys())

    report["duplicate_groups"] = [
        {
            "matched_keys": list(group.matched_keys),
            "canonical_id": group.canonical_id,
            "duplicate_ids": list(group.duplicate_ids),
            "all_ids": list(group.all_ids),
            "candidates": [
                score_snapshot(company_by_id[item_id])
                for item_id in sorted(group.all_ids)
                if item_id in company_by_id
            ],
        }
        for group in duplicate_groups
    ]

    order_items_raw = fetch_all_items(
        client=client,
        board_id=args.orders_board_id,
        column_ids=list(RELATION_COLUMNS),
        page_limit=args.page_limit,
    )
    migration_actions, affected_orders_by_duplicate = plan_relation_migration(order_items_raw, duplicate_to_canonical)

    # Calculate additional counts
    companies_items_total_analyzed = len(companies)
    companies_duplicate_items_identified = len(duplicate_ids)
    companies_items_in_duplicate_groups = sum(len(g.all_ids) for g in duplicate_groups)
    companies_duplicate_ratio = round(companies_duplicate_items_identified / companies_items_total_analyzed, 6) if companies_items_total_analyzed > 0 else 0.0
    orders_items_total_analyzed = len(order_items_raw)

    report["analysis_counts"] = {
        "companies_items_total_analyzed": companies_items_total_analyzed,
        "companies_duplicate_items_identified": companies_duplicate_items_identified,
        "companies_items_in_duplicate_groups": companies_items_in_duplicate_groups,
        "companies_duplicate_ratio": companies_duplicate_ratio,
        "orders_items_total_analyzed": orders_items_total_analyzed
    }

    # Identify Unique Affected Orders
    affected_order_ids_unique = sorted(list(set(action.order_id for action in migration_actions)))
    report["affected_orders"] = {
        "unique_order_ids_count": len(affected_order_ids_unique),
        "unique_order_ids": affected_order_ids_unique
    }

    dry_run_rows = []
    for duplicate_id in duplicate_ids:
        order_ids = affected_orders_by_duplicate.get(duplicate_id, [])
        dry_run_rows.append(
            {
                "duplicate_id": duplicate_id,
                "canonical_id": duplicate_to_canonical[duplicate_id],
                "affected_orders_count": len(order_ids),
                "affected_order_ids": order_ids,
            }
        )
    report["dry_run_impacts"] = dry_run_rows
    report["planned_relation_updates_count"] = len(migration_actions)
    report["planned_relation_updates"] = [
        {
            "order_id": action.order_id,
            "column_id": action.column_id,
            "from_company_id": action.from_company_id,
            "to_company_id": action.to_company_id,
        }
        for action in migration_actions
    ]

    if args.mode == "dry-run":
        report["verification"] = {
            "performed": False,
            "passed": None,
            "remaining_references": [],
        }
        report["archived_duplicates"] = []
        report["summary"] = {
            "duplicate_groups_count": len(duplicate_groups),
            "duplicate_items_count": len(duplicate_ids),
            "orders_to_update_count": len(migration_actions),
            "mutations_executed": 0,
            "archives_executed": 0,
        }
        return report

    # EXECUTION MODE
    relation_update_log = execute_relation_updates(
        client=client,
        actions=migration_actions,
        orders_board_id=args.orders_board_id,
    )
    report["executed_relation_updates"] = relation_update_log

    verification_scan = fetch_all_items(
        client=client,
        board_id=args.orders_board_id,
        column_ids=list(RELATION_COLUMNS),
        page_limit=args.page_limit,
    )
    remaining_references = verify_no_duplicate_references(verification_scan, duplicate_ids)
    report["verification"] = {
        "performed": True,
        "passed": len(remaining_references) == 0,
        "remaining_references": remaining_references,
    }
    if remaining_references:
        raise SafetyCheckError(
            "Verification failed: duplicate references still exist in orders board. "
            "Archiving is blocked to prevent relation loss."
        )

    archived_duplicates = []
    if args.archive_duplicates:
        archived_duplicates = archive_duplicates(client, duplicate_ids)
    report["archived_duplicates"] = archived_duplicates
    report["summary"] = {
        "duplicate_groups_count": len(duplicate_groups),
        "duplicate_items_count": len(duplicate_ids),
        "orders_to_update_count": len(migration_actions),
        "mutations_executed": len(relation_update_log),
        "archives_executed": len(archived_duplicates),
    }
    return report


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Safe and deterministic monday.com company deduplication.")
    parser.add_argument("--mode", choices=("dry-run", "execute"), default="dry-run")
    parser.add_argument("--api-token", default=None, help="monday.com API token (or MONDAY_API_TOKEN env var).")
    parser.add_argument("--api-url", default=MONDAY_API_URL)

    parser.add_argument("--companies-board-id", type=int, default=COMPANIES_BOARD_ID)
    parser.add_argument("--orders-board-id", type=int, default=ORDERS_BOARD_ID)
    parser.add_argument("--contacts-board-id", type=int, default=CONTACTS_BOARD_ID)

    parser.add_argument("--page-limit", type=int, default=500)
    parser.add_argument("--max-retries", type=int, default=5)
    parser.add_argument("--backoff-seconds", type=float, default=1.0)
    parser.add_argument("--timeout-seconds", type=float, default=60.0)
    
    parser.add_argument(
        "--max-duplicate-groups",
        type=int,
        default=0,
        help="Maximum number of duplicate groups to process (0 = unlimited).",
    )

    parser.add_argument(
        "--archive-duplicates",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Archive duplicates after verification in execute mode.",
    )
    return parser


def main() -> int:
    parser = build_arg_parser()
    args = parser.parse_args()
    try:
        report = run(args)
        print(json.dumps(report, indent=2, ensure_ascii=True))
        return 0
    except (MondayApiError, SafetyCheckError) as error:
        error_report = {
            "mode": args.mode,
            "timestamp_utc": dt.datetime.now(dt.timezone.utc).isoformat(),
            "error_type": error.__class__.__name__,
            "error_message": str(error),
        }
        print(json.dumps(error_report, indent=2, ensure_ascii=True), file=sys.stderr)
        return 2


if __name__ == "__main__":
    sys.exit(main())
